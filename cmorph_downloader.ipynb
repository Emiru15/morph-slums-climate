{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import shutil\n",
    "import re\n",
    "import calendar\n",
    "import os\n",
    "from datetime import date\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def day_iter(year, month):\n",
    "    for i in range(1, calendar.monthrange(year, month)[1] + 1):\n",
    "        yield i\n",
    "\n",
    "def construct_path(year, month, day):\n",
    "    return f'https://www.ncei.noaa.gov/data/cmorph-high-resolution-global-precipitation-estimates/access/30min/8km/{year}/{month:02d}/{day:02d}/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook version is more stable but you cannot run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_list = range(1998, 2024)\n",
    "month_list = range(1, 13)\n",
    "\n",
    "for year in year_list:\n",
    "   for month in month_list:\n",
    "        for day in day_iter(year, month):\n",
    "            \n",
    "  # target page containing links to the image files\n",
    "            target_page = construct_path(year, month, day)\n",
    "            print(target_page)\n",
    "\n",
    "  # local path\n",
    "            dest_path = f'/home/emir/cmorph/{year}/{month:02d}/{day:02d}/'\n",
    "            if not os.path.isdir(dest_path):\n",
    "                os.makedirs(dest_path)\n",
    "            # NOTE: this implementation (easily modified) assumes link hrefs contain absolute\n",
    "            # URL's with 'http://' protocol prefix e.g. http://example.com/dir/image.jpg and that \n",
    "            # all links on the target_page point to desired image files.\n",
    "\n",
    "            img_urls = []\n",
    "\n",
    "            page = urllib.request.urlopen(target_page).read()\n",
    "            soup = BeautifulSoup(page, 'html.parser')\n",
    "            # print(soup)\n",
    "            for link in soup.findAll('a', attrs={'href': re.compile('_(\\d+)\\.nc$')}):\n",
    "                print(link)\n",
    "                img_urls.append(link.get('href'))\n",
    "\n",
    "            counter = 1\n",
    "\n",
    "            for img_url in img_urls:\n",
    "\n",
    "                img_filename = Path(img_url).name\n",
    "                img_dest = dest_path + '/' + img_filename\n",
    "\n",
    "                # recreate url with a url-encoded img_filename to handle whitespace in filenames\n",
    "                # img_url_clean = img_url.rsplit('/', 1)[0] + '/' + urllib.parse.quote(img_filename)\n",
    "                img_url_clean = img_url.rsplit('/', 1)[0] + '/' + img_filename\n",
    "                img_url_clean = target_page+ img_url_clean.split('/')[0]\n",
    "                print(\"clean\", img_url_clean)\n",
    "                print(str(counter) + \":\\t \" + img_dest)\n",
    "                counter += 1\n",
    "                \n",
    "                # urlretrieve(img_url_clean, img_dest)\n",
    "                request = requests.get(img_url_clean, timeout=10, stream=True)\n",
    "\n",
    "                # Open the output file and make sure we write in binary mode\n",
    "                with open(img_dest, 'wb') as fh:\n",
    "                    # Walk through the request response in chunks of 1024 * 1024 bytes, so 1MiB\n",
    "                    for chunk in request.iter_content(1024 * 1024):\n",
    "                        # Write the chunk to the file\n",
    "                        fh.write(chunk)\n",
    "                        # Optionally we can check here if the download is taking too long\n",
    "\n",
    "            print(\"DONE!\")\n",
    "            print(\"Saved \" + str(counter - 1) + \" files.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
