{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib as mpl\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from skimage import exposure\n",
    "import numba\n",
    "import umap\n",
    "from sklearn.cluster import KMeans, SpectralClustering, DBSCAN, OPTICS, Birch, BisectingKMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from osgeo import gdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iic.datasets import SateliteDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR_LANDSAT = \"data/portharcourt_images\"\n",
    "BASE_DIR_LANDCOVER = \"data/portharcourt_images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-16\n",
    "\n",
    "\n",
    "\n",
    "def compute_NDVI(img, watermask=None, c=1, percentile=10):\n",
    "    nir = img[:, :, :, 3]\n",
    "    p_min, p_max = np.percentile(nir, (percentile, 100.0-percentile))\n",
    "    nir = exposure.rescale_intensity(nir, in_range=(p_min, p_max), out_range=(0 + EPS, 1 - EPS))\n",
    "\n",
    "        \n",
    "    red = img[:, :, :, 2]\n",
    "    p_min, p_max = np.percentile(red, (percentile, 100.0-percentile))\n",
    "    red = exposure.rescale_intensity(red, in_range=(p_min, p_max), out_range=(0 + EPS, 1 - EPS))\n",
    "    \n",
    "    \n",
    "    if watermask is not None:\n",
    "        mask = watermask[:, :, :, 0] == 0\n",
    "        red[mask] = 1-EPS \n",
    "        nir[mask] = EPS\n",
    "    \n",
    "    out = (nir - red)/(nir + red)\n",
    "\n",
    "    p_min, p_max = np.percentile(out, (2, 100.0-2))\n",
    "    out = exposure.rescale_intensity(out, in_range=(p_min, p_max), out_range=(-1, 1))\n",
    "    return out[:, :, :, None]\n",
    "\n",
    "\n",
    "def compute_NDBI(img, watermask=None, percentile=10):\n",
    "    nir = img[:, :, :, 3]\n",
    "    p_min, p_max = np.percentile(nir, (percentile, 100.0-percentile))\n",
    "    nir = exposure.rescale_intensity(nir, in_range=(p_min, p_max), out_range=(0 + EPS, 1 - EPS))\n",
    "\n",
    "    swir = img[:, :, :, 4]\n",
    "    p_min, p_max = np.percentile(swir, (percentile, 100.0-percentile))\n",
    "    swir = exposure.rescale_intensity(swir, in_range=(p_min, p_max), out_range=(0 + EPS, 1 - EPS))\n",
    "\n",
    "    if watermask is not None:\n",
    "        mask = watermask[:, :, :, 0] == 0\n",
    "        nir[mask] = 1-EPS \n",
    "        swir[mask] = EPS\n",
    "\n",
    "    out = ((swir - nir)/(swir + nir))\n",
    "    p_min, p_max = np.percentile(out, (2, 100.0-2))\n",
    "    out = exposure.rescale_intensity(out, in_range=(p_min, p_max), out_range=(-1, 1))\n",
    "    return out[:, :, :, None]\n",
    "\n",
    "\n",
    "def compute_BU(ndvi, ndbi, percentile=1):\n",
    "    out = ndbi-ndvi\n",
    "    p_min, p_max = np.percentile(out, (percentile, 100.0-percentile))\n",
    "    out = exposure.rescale_intensity(out, in_range=(p_min, p_max), out_range=(0, 1))\n",
    "    return out\n",
    "\n",
    "\n",
    "def extract_water(img):\n",
    "    p_min, p_max = np.percentile(img, (5, 95))\n",
    "    img = exposure.rescale_intensity(img, in_range=(p_min, p_max), out_range=(0, 1))\n",
    "    img = exposure.adjust_gamma(img, gamma=0.001)\n",
    "    return img\n",
    "\n",
    "\n",
    "def get_water_mask(image_data, percentile=5):\n",
    "    waterbands = np.where((image_data== 21888.0) | (image_data == 21952.0), 0, 1)\n",
    "    print(waterbands.shape)\n",
    "    return waterbands[:, :, :, 7:8]\n",
    "\n",
    "def compute_water_index(image_data, percentile=10):\n",
    "    green = image_data[:,:,:,1]\n",
    "    p_min, p_max = np.percentile(green, (percentile, 100.0-percentile))\n",
    "    green = exposure.rescale_intensity(green, in_range=(p_min, p_max), out_range=(0 + EPS, 1 - EPS))\n",
    "\n",
    "    swir1 = image_data[:,:,:,4]\n",
    "    p_min, p_max = np.percentile(swir1, (percentile, 100.0-percentile))\n",
    "    swir1 = exposure.rescale_intensity(swir1, in_range=(p_min, p_max), out_range=(0 + EPS, 1 - EPS))\n",
    "\n",
    "    out = (green - swir1) / (green + swir1)\n",
    "    p_min, p_max = np.percentile(out, (percentile, 100.0-percentile))\n",
    "    out = exposure.rescale_intensity(out, in_range=(p_min, p_max), out_range=(-1, 1))\n",
    "    \n",
    "    return out[:, :, :, None]\n",
    "\n",
    "def compute_NDISI(image_data, percentile=10):\n",
    "    tir1 = image_data[:,:,:,6]\n",
    "    p_min, p_max = np.percentile(tir1, (percentile, 100.0-percentile))\n",
    "    tir1 = exposure.rescale_intensity(tir1, in_range=(p_min, p_max), out_range=(0 + EPS, 1 - EPS))\n",
    "    tir1 = tir1[:,:,:,None]\n",
    "\n",
    "    nir = image_data[:, :, :, 3]\n",
    "    p_min, p_max = np.percentile(nir, (percentile, 100.0-percentile))\n",
    "    nir = exposure.rescale_intensity(nir, in_range=(p_min, p_max), out_range=(0 + EPS, 1 - EPS))\n",
    "    nir = nir[:,:,:,None]\n",
    "\n",
    "    swir1 = image_data[:,:,:,4]\n",
    "    p_min, p_max = np.percentile(swir1, (percentile, 100.0-percentile))\n",
    "    swir1 = exposure.rescale_intensity(swir1, in_range=(p_min, p_max), out_range=(0 + EPS, 1 - EPS))\n",
    "    swir1 = swir1[:,:,:,None]\n",
    "    \n",
    "    wi = compute_water_index(image_data)\n",
    "\n",
    "    out = (tir1 - ((wi+nir+swir1)/3)) / (tir1 + ((wi+nir+swir1)/3))\n",
    "    p_min, p_max = np.percentile(out, (percentile, 100.0-percentile))\n",
    "    out = exposure.rescale_intensity(out, in_range=(p_min, p_max), out_range=(-1, 1))\n",
    "\n",
    "    return out\n",
    "\n",
    "def normalize_band(img, band, percentile):\n",
    "    p_min, p_max = np.percentile(img[:, :, :, band], (percentile, 100.0-percentile))\n",
    "    img[:, :, :, band] = exposure.rescale_intensity(img[:, :, :, band], in_range=(p_min, p_max), out_range=(0, 1))\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def normalize_percentile(image_data, percentile):\n",
    "    for year in range(image_data.shape[0]):\n",
    "        for ch in range(0, image_data.shape[-1]):\n",
    "            image_data = normalize_band(image_data, ch, percentile)\n",
    "\n",
    "    return image_data\n",
    "\n",
    "\n",
    "def preprocess_data(image_data):\n",
    "\n",
    "\n",
    "    image_data_p_1 = normalize_percentile(image_data.copy(), 2)\n",
    "    waterbands = get_water_mask(image_data)\n",
    "    ndvi_band = compute_NDVI(image_data.copy(), watermask=waterbands)\n",
    "    ndbi_band = compute_NDBI(image_data.copy(), watermask=waterbands)\n",
    "    bu_index = compute_BU(ndvi_band, ndbi_band)\n",
    "    ndisi_band = compute_NDISI(image_data.copy())\n",
    "    water_index = compute_water_index(image_data.copy())\n",
    "\n",
    "  \n",
    "    img_out = np.concatenate([image_data_p_1, waterbands, ndvi_band, ndbi_band, bu_index, water_index, ndisi_band], axis=-1)\n",
    "    return img_out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_satelite_image(path):\n",
    "  img = np.load(path)\n",
    "  img = img.transpose(0, 2, 3, 1) # year, lat, lon, band\n",
    "  for year in range(img.shape[0]):\n",
    "    t_year = img[year]\n",
    "    pos_inf = t_year[np.isfinite(t_year)].max()\n",
    "    neg_inf = t_year[np.isfinite(t_year)].min()\n",
    "    t_year = np.nan_to_num(t_year, nan=0.0, posinf=pos_inf, neginf=neg_inf)\n",
    "    np.where(np.abs(t_year) >= 2, np.median(t_year), t_year)\n",
    "    img[year] = t_year\n",
    "  return img\n",
    "\n",
    "def plot_all_bands(img_mat, year, year_name=2013, band_title_map=None):\n",
    "    columns = int(np.ceil(np.sqrt(img_mat.shape[-1])))\n",
    "    rows = int(np.ceil(np.sqrt(img_mat.shape[-1])))\n",
    "    print(columns*rows, img_mat.shape[-1])\n",
    "    fig = plt.figure(figsize=(5*columns, 5*rows))\n",
    "    for i in range(1, img_mat.shape[-1]+1):\n",
    "        img = img_mat[year, :, :, i-1]\n",
    "        ax = fig.add_subplot(rows, columns, i)\n",
    "        aa = ax.matshow(img)\n",
    "        fig.colorbar(aa, ax=ax)\n",
    "        if band_title_map is None:\n",
    "          ax.set_title(f\"Band {i-1}\")\n",
    "        else:\n",
    "          ax.set_title(band_title_map[i-1])\n",
    "        ax.get_xaxis().set_ticks([])\n",
    "        ax.get_yaxis().set_ticks([])\n",
    "    #fig.suptitle(f\"Year: {year_name}\"\n",
    "    plt.show()\n",
    "\n",
    "def get_silhouette(labels, image):\n",
    "    for i, k in enumerate(range(2, 8)):\n",
    "       \n",
    "        bisecting = BisectingKMeans(n_clusters=k, random_state=420)\n",
    "        labels = bisecting.fit_predict(image)\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        fig.set_size_inches(18, 7)\n",
    "        \n",
    "        silhouette = silhouette_samples(image, labels)\n",
    "        \n",
    "        y_ticks = []\n",
    "        y_lower, y_upper = 0, 0\n",
    "        for i, cluster in enumerate(np.unique(labels)):\n",
    "            \n",
    "            cluster_silhouette_vals = silhouette[labels == cluster]\n",
    "            cluster_silhouette_vals.sort()\n",
    "            y_upper += len(cluster_silhouette_vals)\n",
    "            ax1.barh(range(y_lower, y_upper), cluster_silhouette_vals, edgecolor='none', height=1)\n",
    "            ax1.text(-0.03, (y_lower + y_upper) / 2, str(i + 1))\n",
    "            y_lower += len(cluster_silhouette_vals)\n",
    "\n",
    "        # Get the average silhouette score and plot it\n",
    "        avg_score = np.mean(silhouette)\n",
    "        ax1.axvline(avg_score, linestyle='--', linewidth=2, color='green')\n",
    "        ax1.set_yticks([])\n",
    "        ax1.set_xlim([-0.1, 1])\n",
    "        ax1.set_xlabel('Silhouette coefficient values')\n",
    "        ax1.set_ylabel('Cluster labels')\n",
    "        ax1.set_title('Silhouette plot for the various clusters', y=1.02)\n",
    "\n",
    "    return silhouette\n",
    "\n",
    "def cluster_kmeans(k, image):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=420).fit(image)\n",
    "\n",
    "    return kmeans\n",
    "\n",
    "def cluster_bisecting_kmeans(k, image):\n",
    "    bisecting = BisectingKMeans(n_clusters=k, random_state=420)\n",
    "    labels = bisecting.fit_predict(image)\n",
    "    return bisecting, labels\n",
    "\n",
    "def cluster_spectral(k, image):\n",
    "    spectral = SpectralClustering(n_clusters=k, random_state=420).fit(image)\n",
    "\n",
    "    return spectral\n",
    "\n",
    "def cluster_dbscan(eps, min_samples, image):\n",
    "   dbscan = DBSCAN(eps=eps, min_samples=min_samples, algorithm=\"ball_tree\", metric=\"manhattan\").fit(image)\n",
    "\n",
    "   return dbscan\n",
    "\n",
    "def cluster_optics(max_eps, min_samples, image):\n",
    "   optics = OPTICS(max_eps=max_eps, min_samples=min_samples, xi=0.008).fit(image)\n",
    "\n",
    "   return optics\n",
    "\n",
    "def map_kmeans(k, kmeans_data, image_map, data_indices, title=\"Kmeans\"):\n",
    "    idx = np.argsort(kmeans_data.cluster_centers_.sum(axis=1))\n",
    "    lut = np.zeros_like(idx)\n",
    "    lut[idx] = np.arange(k)\n",
    "\n",
    "    for i, (rows, cols) in enumerate(data_indices):\n",
    "        image_map[:, rows, cols] = lut[kmeans_data.labels_[i]] + 1\n",
    "\n",
    "    plt.matshow(LABELS)\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Ground Truth\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.matshow(image_map[0, :, :], cmap=mpl.colormaps['turbo'])\n",
    "    plt.colorbar()\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def map_optics(optics_data, image_map, data_indices, title=\"Optics\"):\n",
    "    # idx = np.argsort(optics_data.cluster_centers_.sum(axis=1))\n",
    "    # lut = np.zeros_like(idx)\n",
    "    # lut[idx] = np.arange(k)\n",
    "\n",
    "    for i, (rows, cols) in enumerate(data_indices):\n",
    "        image_map[:, rows, cols] = optics_data.labels_[i]\n",
    "\n",
    "    plt.matshow(LABELS)\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Ground Truth\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.matshow(image_map[0, :, :], cmap=mpl.colormaps['turbo'])\n",
    "    plt.colorbar()\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def raster_download (file_name, raster):\n",
    "\n",
    "    ds = gdal.Open(f\"{BASE_DIR_LANDSAT}/image_2020.tif\")\n",
    "    band = ds.GetRasterBand(2)\n",
    "    arr = raster\n",
    "    [cols, rows] = arr.shape\n",
    "    print(arr.shape)\n",
    "    format = \"GTiff\"\n",
    "    driver = gdal.GetDriverByName(format)\n",
    "\n",
    "\n",
    "    outDataRaster = driver.Create(f\"{BASE_DIR_LANDSAT}/{file_name}\", rows, cols, 1, gdal.GDT_Float32)\n",
    "    outDataRaster.SetGeoTransform(ds.GetGeoTransform())##sets same geotransform as input\n",
    "    outDataRaster.SetProjection(ds.GetProjection())##sets same projection as input\n",
    "\n",
    "\n",
    "    outDataRaster.GetRasterBand(1).WriteArray(raster)\n",
    "\n",
    "    outDataRaster.FlushCache() ## remove from memory\n",
    "    del outDataRaster ## delete the data (not the actual geotiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PANCHROMATIC = load_satelite_image(\"data/nairobi_images_summer_raw/data_raw.npy\")\n",
    "\n",
    "PANCHROMATIC = normalize_percentile(PANCHROMATIC.copy(), 2)\n",
    "PANCHROMATIC = PANCHROMATIC[:8, :, :, -1, None]\n",
    "\n",
    "print(PANCHROMATIC.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "IMAGE_RAW = load_satelite_image(f\"{BASE_DIR_LANDSAT}/data_raw.npy\") # year, lat, lon, band\n",
    "\n",
    "IMAGE_RAW = preprocess_data(IMAGE_RAW)\n",
    "#IMAGE_RAW = np.append(IMAGE_RAW, PANCHROMATIC, axis=3)\n",
    "\n",
    "BANDS = IMAGE_RAW.shape[-1]\n",
    "\n",
    "\n",
    "LABELS = np.load(f\"{BASE_DIR_LANDCOVER}/worldcover_adj_classes.npy\")\n",
    "print(IMAGE_RAW.shape)\n",
    "print(LABELS.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(LABELS, return_counts=True))\n",
    "plot_all_bands(IMAGE_RAW[:, :, :, :], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_download('kampala_mndwi.tif', IMAGE_RAW[-1, :, :, 12])\n",
    "raster_download('kampala_ndisi.tif', IMAGE_RAW[-1, :, :, 13])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BANDS_USED = [0, 1, 2, 3, 4, 5, 9, 10, 11]\n",
    "BLANK_INDICES_ROWS, BLANK_INDICES_COLS = np.where(LABELS != 50)\n",
    "\n",
    "IMAGE_MAP = IMAGE_RAW[-2, :, :, BANDS_USED]\n",
    "print(IMAGE_MAP.shape)\n",
    "\n",
    "for i, j in zip(BLANK_INDICES_ROWS, BLANK_INDICES_COLS):\n",
    "    IMAGE_MAP[:, i, j] = np.nan\n",
    "\n",
    "BUILD_INDICES_ROWS, BUILD_INDICES_COLS = np.where(LABELS == 50)\n",
    "\n",
    "IMAGE_BUILD = IMAGE_RAW[-2,  LABELS == 50, :]\n",
    "IMAGE_UMAP = IMAGE_BUILD[:, BANDS_USED]\n",
    "print(IMAGE_UMAP.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downprojection with UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to check for correlation of data before actually clustering\n",
    "\n",
    "# weight_matrix = np.asarray([\n",
    "#     [1.0, 1.0, 1.0],\n",
    "#     [1.0, 20.0, 1.0],\n",
    "#     [1.0, 1.0, 1.0]\n",
    "# ])\n",
    "\n",
    "# weight_matrix = np.dstack([weight_matrix]*len(BANDS_USED))\n",
    "\n",
    "# @numba.njit()\n",
    "# def sparse_eucliden_dist(a, b):\n",
    "#     a = a.reshape(3, 3, len(BANDS_USED))\n",
    "#     b = b.reshape(3, 3, len(BANDS_USED))\n",
    "#     dist = ((a-b)*weight_matrix).flatten()\n",
    "#     return np.linalg.norm((dist).flatten(), 1)\n",
    "\n",
    "# u = umap.UMAP(n_neighbors=30,\n",
    "#                 min_dist=0.1,\n",
    "#                       metric='euclidean').fit_transform(IMAGE_UMAP)\n",
    "\n",
    "# print(u.shape)\n",
    "\n",
    "# plt.scatter(u[:, 0], u[:, 1])\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering with KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=5\n",
    "kmeans, labels = cluster_bisecting_kmeans(k, IMAGE_UMAP)\n",
    "map_kmeans(k, kmeans, IMAGE_MAP, zip(BUILD_INDICES_ROWS, BUILD_INDICES_COLS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(labels))\n",
    "get_silhouette(labels, IMAGE_UMAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_download('bisecting_5.tif', IMAGE_MAP[0, : ,:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Kmeans on only the \"Slum Cluster\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_cluster = np.max(kmeans.labels_)\n",
    "print(largest_cluster)\n",
    "IMAGE_MAP_SLUM = IMAGE_MAP\n",
    "NON_SLUM_INDICES_ROWS, NON_SLUM_INDICES_COLS = np.where(IMAGE_MAP_SLUM[0, :, :] != largest_cluster)\n",
    "print(np.array(NON_SLUM_INDICES_ROWS).shape)\n",
    "\n",
    "for i, j in zip(NON_SLUM_INDICES_ROWS, NON_SLUM_INDICES_COLS):\n",
    "    IMAGE_MAP_SLUM[:, i, j] = np.nan\n",
    "\n",
    "SLUM_INDICES_ROWS, SLUM_INDICES_COLS = np.where(IMAGE_MAP_SLUM[0, :, :] == largest_cluster)\n",
    "SLUM_INDICES = zip(SLUM_INDICES_ROWS, SLUM_INDICES_COLS)\n",
    "print(np.array(SLUM_INDICES_ROWS).shape)\n",
    "\n",
    "IMAGE_SLUMS = IMAGE_RAW[-2, IMAGE_MAP_SLUM[0, :, :] == largest_cluster, :]\n",
    "IMAGE_SLUMS = IMAGE_SLUMS[:, BANDS_USED]\n",
    "\n",
    "print(IMAGE_SLUMS.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=3\n",
    "kmeans_slums = cluster_bisecting_kmeans(k, IMAGE_SLUMS)\n",
    "map_kmeans(k, kmeans_slums, IMAGE_MAP_SLUM, SLUM_INDICES, \"KMeans Slums\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_download('bisecting_3_slum.tif', IMAGE_MAP_SLUM[0, : ,:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_cluster = np.max(kmeans_slums.labels_)\n",
    "print(largest_cluster)\n",
    "IMAGE_MAP_SLUM_2 = IMAGE_MAP_SLUM\n",
    "NON_SLUM_INDICES_ROWS, NON_SLUM_INDICES_COLS = np.where(IMAGE_MAP_SLUM_2[0, :, :] != largest_cluster)\n",
    "print(np.array(NON_SLUM_INDICES_ROWS).shape)\n",
    "\n",
    "for i, j in zip(NON_SLUM_INDICES_ROWS, NON_SLUM_INDICES_COLS):\n",
    "    IMAGE_MAP_SLUM_2[:, i, j] = np.nan\n",
    "\n",
    "SLUM_INDICES_ROWS, SLUM_INDICES_COLS = np.where(IMAGE_MAP_SLUM_2[0, :, :] == largest_cluster)\n",
    "SLUM_INDICES = zip(SLUM_INDICES_ROWS, SLUM_INDICES_COLS)\n",
    "print(np.array(SLUM_INDICES_ROWS).shape)\n",
    "\n",
    "IMAGE_SLUMS_2 = IMAGE_RAW[-2, IMAGE_MAP_SLUM_2[0, :, :] == largest_cluster, :]\n",
    "IMAGE_SLUMS_2 = IMAGE_SLUMS_2[:, BANDS_USED]\n",
    "\n",
    "print(IMAGE_SLUMS_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=2\n",
    "kmeans_slums_2 = cluster_bisecting_kmeans(k, IMAGE_SLUMS_2)\n",
    "map_kmeans(k, kmeans_slums_2, IMAGE_MAP_SLUM_2, SLUM_INDICES, \"KMeans Slums\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering using IIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUT_OFF = 10\n",
    "TRAIN_IMAGE = IMAGE_RAW[0:-1, CUT_OFF:-CUT_OFF, CUT_OFF:-CUT_OFF, BANDS_USED]\n",
    "IMAGE = IMAGE_RAW[-2:-1, CUT_OFF:-CUT_OFF, CUT_OFF:-CUT_OFF, BANDS_USED]\n",
    "LABELS = LABELS[CUT_OFF:-CUT_OFF, CUT_OFF:-CUT_OFF]\n",
    "BANDS = TRAIN_IMAGE.shape[-1]\n",
    "\n",
    "print(TRAIN_IMAGE.shape, IMAGE.shape, LABELS.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from iic.datasets import SateliteDataset\n",
    "from iic.transforms import RandomVFlipTransformIIC, RandomHFlipTransformIIC\n",
    "from iic.models import IICModel\n",
    "from utils.data_preprocessing import segment_satelite_image, recombine_image\n",
    "\n",
    "\n",
    "\n",
    "def create_image_dataset(image, sub_image_size):\n",
    "    \"\"\"\n",
    "    Given a series of images: (Y, W, H, CH) create a dataset:\n",
    "    (B, ch, sub_image, sub_imahe, )\n",
    "    \"\"\"\n",
    "    ds = []\n",
    "    recombination_size = None\n",
    "    for year in range(image.shape[0]):\n",
    "        img_ = image[0].transpose(2, 0, 1)\n",
    "        seg_images, recombination_size_ = segment_satelite_image(img_, sub_size=sub_image_size)\n",
    "        \n",
    "        recombination_size = recombination_size_\n",
    "        ground_truth = img_[:, :recombination_size[0]*sub_image_size, :recombination_size[1]*sub_image_size]\n",
    "        assert np.allclose(ground_truth, recombine_image(seg_images, recombination_size_, sub_image_size).numpy())\n",
    "        assert recombination_size is None or recombination_size_ == recombination_size\n",
    "        \n",
    "        ds.append(seg_images)\n",
    "    return torch.concat(ds), recombination_size\n",
    "\n",
    "SUB_IMAGE_SIZE = 16\n",
    "CROP_FACTOR = 1.0\n",
    "\n",
    "\n",
    "TRAIN_DATA, RECOMBINATION_SIZE = create_image_dataset(TRAIN_IMAGE, SUB_IMAGE_SIZE)\n",
    "VALIDATION_DATA, _ = create_image_dataset(IMAGE, SUB_IMAGE_SIZE)\n",
    "print(TRAIN_DATA.shape, VALIDATION_DATA.shape)\n",
    "\n",
    "validation_rec = recombine_image(VALIDATION_DATA, RECOMBINATION_SIZE, SUB_IMAGE_SIZE).numpy()\n",
    "plt.matshow(validation_rec[-1])\n",
    "plt.show()\n",
    "\n",
    "plt.matshow(TRAIN_DATA[0, -1])\n",
    "plt.show()\n",
    "\n",
    "print(TRAIN_DATA.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = SateliteDataset(TRAIN_DATA, invariant_transforms=[RandomVFlipTransformIIC(0.5), RandomHFlipTransformIIC(0.5)])\n",
    "val_set = SateliteDataset(VALIDATION_DATA, invariant_transforms=[])\n",
    "\n",
    "# train_size = int(len(train_set)*0.9)\n",
    "# val_size = len(train_set) - train_size\n",
    "print(train_set[0:1][\"img1\"].shape)\n",
    "print(train_set[0:1][\"img2\"].shape)\n",
    "print(len(train_set), len(val_set))\n",
    "\n",
    "plt.matshow(train_set[0:1][\"img1\"][0, -1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IICModel(BANDS, 4, int(SUB_IMAGE_SIZE*CROP_FACTOR))\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_set, batch_size=32, num_workers=2)\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10, devices=1)\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO tune hyperparams on big device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cpu()\n",
    "\n",
    "SEG_IMAGE_OUT, RECOMBINATION_SIZE_OUT = segment_satelite_image(IMAGE, sub_size=int(SUB_IMAGE_SIZE*CROP_FACTOR))\n",
    "print(RECOMBINATION_SIZE_OUT)\n",
    "#VALIDATION_SET\n",
    "sample_clf = model(val_set[0:][\"img1\"]).detach()\n",
    "#sample_clf = model(SEG_IMAGE_OUT).detach()\n",
    "sample_clf = recombine_image(sample_clf, RECOMBINATION_SIZE_OUT, int(SUB_IMAGE_SIZE*CROP_FACTOR))\n",
    "\n",
    "test_classification = torch.argmax(sample_clf, axis=0).detach().numpy()\n",
    "\n",
    "ground_truth_label = LABELS[:RECOMBINATION_SIZE_OUT[0]*int(SUB_IMAGE_SIZE*CROP_FACTOR), :RECOMBINATION_SIZE_OUT[1]*int(SUB_IMAGE_SIZE*CROP_FACTOR)]\n",
    "print(ground_truth_label.shape)\n",
    "print(test_classification.shape)\n",
    "\n",
    "plt.matshow(ground_truth_label)\n",
    "plt.colorbar()\n",
    "plt.title(\"Ground Truth\")\n",
    "plt.show()\n",
    "\n",
    "plt.matshow(test_classification)\n",
    "plt.colorbar()\n",
    "plt.title(\"Classification\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote-sensing-landuse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:46:39) \n[GCC 10.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "39f9f1f065b5771fa159c3adde0966191b6d7c113f68f4d93de249481ae639bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
